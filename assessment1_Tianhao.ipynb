{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BusyApe201804930/assessment/blob/main/assessment1_Tianhao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "intellectual-oregon",
      "metadata": {
        "id": "intellectual-oregon"
      },
      "outputs": [],
      "source": [
        "# not sure if it is ok but using pytorch here instead of tensorflow and keras\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "harmful-newton",
      "metadata": {
        "id": "harmful-newton"
      },
      "outputs": [],
      "source": [
        "# Start of P1\n",
        "train= pd.read_csv('/content/train-1.csv',header=None)\n",
        "train.shape\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "liked-prague",
      "metadata": {
        "id": "liked-prague"
      },
      "outputs": [],
      "source": [
        "test= pd.read_csv('/content/test-1.csv',header=None)\n",
        "train.shape\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "early-technique",
      "metadata": {
        "id": "early-technique"
      },
      "outputs": [],
      "source": [
        "length=int(len(train)*0.1)\n",
        "length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stainless-seminar",
      "metadata": {
        "id": "stainless-seminar"
      },
      "outputs": [],
      "source": [
        "valid=train.iloc[:10000,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "western-roulette",
      "metadata": {
        "id": "western-roulette"
      },
      "outputs": [],
      "source": [
        "# setting the data set without the last column\n",
        "x_train=train.iloc[:,:-1].values\n",
        "y_train=train.iloc[:,-1].values\n",
        "x_valid=valid.iloc[:,:-1].values\n",
        "y_valid=valid.iloc[:,-1].values\n",
        "x_test=test.iloc[:,:-1].values\n",
        "y_test=test.iloc[:,-1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "liberal-third",
      "metadata": {
        "id": "liberal-third"
      },
      "outputs": [],
      "source": [
        "x_train = torch.tensor(x_train, dtype = float)\n",
        "y_train = torch.tensor(y_train, dtype = float)\n",
        "x_valid = torch.tensor(x_valid , dtype = torch.float)\n",
        "y_valid = torch.tensor(y_valid , dtype =torch.float)\n",
        "x_test = torch.tensor(x_test, dtype = torch.float)\n",
        "y_test = torch.tensor(y_test, dtype = torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "moral-bottom",
      "metadata": {
        "id": "moral-bottom"
      },
      "outputs": [],
      "source": [
        "# specify single layer, batch 128 and 100 nods in the NN\n",
        "# activation function using Relu here\n",
        "# also define else for 1.4 use (sigmoid activation)\n",
        "input_size =x_train.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 1\n",
        "batch_size = 128\n",
        "def my_model(lat_dim,input_size, output_size,activation='relu'):\n",
        "    if activation=='relu':\n",
        "        my_nn = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 100),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(100, lat_dim),\n",
        "            torch.nn.Linear(lat_dim, 100),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(100, output_size),\n",
        "        )\n",
        "    else:\n",
        "        my_nn = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 100),\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(100, lat_dim),\n",
        "            torch.nn.Linear(lat_dim, 100),\n",
        "            torch.nn.Sigmoid(),\n",
        "            torch.nn.Linear(100, output_size),\n",
        "        )\n",
        "    return my_nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "serious-annotation",
      "metadata": {
        "id": "serious-annotation"
      },
      "outputs": [],
      "source": [
        "# MSE loss with optimization SGD \n",
        "# test and validation loss here\n",
        "lat_dim=5\n",
        "def train(lat_dim,input_size, output_size,lr,epoch,batch_size,activation='relu',verbose=True):\n",
        "    my_nn=my_model(lat_dim,input_size, output_size,activation='relu')\n",
        "    cost = torch.nn.MSELoss(reduction='mean')\n",
        "    optimizer = torch.optim.SGD(my_nn.parameters(), lr = lr)\n",
        "    losses_valid = []\n",
        "    losses_test = []\n",
        "  \n",
        "\n",
        "    for i in range(epoch):\n",
        "        batch_loss_valid = []\n",
        "        batch_loss_test = []\n",
        "        for start in range(0, len(x_train), batch_size):\n",
        "            end = start + batch_size if start + batch_size < len(x_train) else len(x_train)\n",
        "            xx = torch.tensor(x_train[start:end], dtype = torch.float, requires_grad = True)\n",
        "            yy = torch.tensor(y_train[start:end], dtype = torch.float, requires_grad = True)\n",
        "            prediction = my_nn(xx)\n",
        "            loss = cost(prediction, yy)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "            \n",
        "\n",
        "        prediction_valid = my_nn(x_valid)\n",
        "        l_valid=cost(prediction_valid, y_valid)\n",
        "\n",
        "        prediction_test = my_nn(x_train)\n",
        "        l_train=cost(prediction_test, y_test)\n",
        "\n",
        "\n",
        "        losses_valid.append(l_valid.data.numpy())\n",
        "        losses_test.append(l_test.data.numpy())\n",
        "        if verbose==True:\n",
        "            print('valid loss for epoch '+str(i)+': '+str(l_valid.data.numpy()))\n",
        "            print('test loss for epoch '+str(i)+': '+str(l_test.data.numpy()))\n",
        "    return my_nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparable-cylinder",
      "metadata": {
        "id": "comparable-cylinder"
      },
      "outputs": [],
      "source": [
        "# P1.2 \n",
        "# lat_dim = 5, epouch = 10\n",
        "input_size = x_train.shape[1]\n",
        "lat_dim = 5\n",
        "output_size = 1\n",
        "batch_size = 128\n",
        "lr=1e-4\n",
        "epoch=10\n",
        "my_nn=train(lat_dim,input_size, output_size,lr,epoch,batch_size,activation='relu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "marked-finding",
      "metadata": {
        "id": "marked-finding"
      },
      "outputs": [],
      "source": [
        "# P1.2\n",
        "# lat_dim = 100, epoch = 10\n",
        "input_size =x_train.shape[1]\n",
        "lat_dim = 100\n",
        "output_size = 1\n",
        "batch_size = 128\n",
        "lr=1e-4\n",
        "epoch=10\n",
        "my_nn=train(lat_dim,input_size, output_size,lr,epoch,batch_size,activation='relu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As can be seen from the results, \n",
        "# when lat_dim increases to 100, both valid loss and test loss increase"
      ],
      "metadata": {
        "id": "uNxekXAVK3hh"
      },
      "id": "uNxekXAVK3hh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gorgeous-theorem",
      "metadata": {
        "scrolled": true,
        "id": "gorgeous-theorem"
      },
      "outputs": [],
      "source": [
        "# P1.3\n",
        "# lat_dim = 100, epoch = 10\n",
        "# lr with 0.01, 0.1 and 0.5\n",
        "input_size =x_train.shape[1]\n",
        "lat_dim = 100\n",
        "output_size = 1\n",
        "batch_size = 128\n",
        "lr=[0.01,0.1,0.5]\n",
        "epoch=10\n",
        "mse_test=[]\n",
        "for i in lr:\n",
        "    my_nn=train(lat_dim,input_size, output_size,i,epoch,batch_size,activation='relu')\n",
        "    prediction_test = my_nn(x_test)\n",
        "    m=np.mean((prediction_test.data.numpy()-y_test.data.numpy())**2)\n",
        "    mse_test.append(m)\n",
        "mse_test\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss should decrease as lr increases"
      ],
      "metadata": {
        "id": "Bl8qEXIKLkZu"
      },
      "id": "Bl8qEXIKLkZu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "portable-fellow",
      "metadata": {
        "scrolled": true,
        "id": "portable-fellow"
      },
      "outputs": [],
      "source": [
        "# P1.4\n",
        "# lat_dim = 100, epoch = 50\n",
        "input_size =x_train.shape[1]\n",
        "lat_dim = 100\n",
        "output_size = 1\n",
        "batch_size = 128\n",
        "lr=1e-4\n",
        "epoch=50\n",
        "my_nn=train(lat_dim,input_size, output_size,lr,epoch,batch_size,activation='relu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "architectural-congo",
      "metadata": {
        "id": "architectural-congo"
      },
      "outputs": [],
      "source": [
        "# for sigmoid\n",
        "input_size =x_train.shape[1]\n",
        "lat_dim = 100\n",
        "output_size = 1\n",
        "batch_size = 128\n",
        "lr=1e-4\n",
        "epoch=50\n",
        "my_nn=train(lat_dim,input_size, output_size,lr,epoch,batch_size,activation='sigmoid')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss is better for relu, sigmoid performs worse in this case"
      ],
      "metadata": {
        "id": "-7dNEsM5L320"
      },
      "id": "-7dNEsM5L320",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "empirical-renaissance",
      "metadata": {
        "id": "empirical-renaissance"
      },
      "outputs": [],
      "source": [
        "list(range(1,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mexican-fairy",
      "metadata": {
        "id": "mexican-fairy"
      },
      "outputs": [],
      "source": [
        "# P1.5\n",
        "# Change lat_dim size with different lr, with epoch = 5, batch size = 128\n",
        "lat_dim=list(range(1,10))+list(range(10,110,10))\n",
        "output_size = 1\n",
        "batch_size = 128\n",
        "lr=[0.01,0.02,0.03,0.04,0.1]\n",
        "epoch=5\n",
        "mse_total=[]\n",
        "for j in lat_dim:\n",
        "    mse_valid=[]\n",
        "    model=[]\n",
        "    mse_test=[]\n",
        "    for i in lr:\n",
        "        my_nn=train(j,input_size, output_size,i,epoch,batch_size,activation='relu',verbose=False)\n",
        "        prediction_valid = my_nn(x_valid)\n",
        "        m=np.mean((prediction_valid.data.numpy()-y_valid.data.numpy())**2)\n",
        "        mse_valid.append(m)\n",
        "        prediction_test = my_nn(x_test)\n",
        "        m=np.mean((prediction_test.data.numpy()-y_test.data.numpy())**2)\n",
        "        mse_test.append(m)\n",
        "        model.append(my_nn)\n",
        "    mse_total.append(np.mean(mse_test))\n",
        "    best_model=model[mse_valid.index(max(mse_valid))]\n",
        "    prediction_test =best_model(x_test)\n",
        "    m=np.mean((prediction_test.data.numpy()-y_test.data.numpy())**2)\n",
        "    print(str(j)+': '+str(m))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "marine-american",
      "metadata": {
        "id": "marine-american"
      },
      "outputs": [],
      "source": [
        "# plotting lat_dim against MSE here\n",
        "# as lat_dim increases a little bit, mse decreases dramatically\n",
        "# however as lat_dim grows, no big effect on mse, and when it is very large, mse tends to increase\n",
        "plt.plot(lat_dim,mse_total)\n",
        "plt.xlabel('lat_dim')\n",
        "plt.ylabel('mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "expired-basket",
      "metadata": {
        "id": "expired-basket"
      },
      "outputs": [],
      "source": [
        "# Start of P2\n",
        "x_train= pd.read_csv('/content/xtrain-2.csv').drop('Unnamed: 0',axis=1)\n",
        "y_train= pd.read_csv('/content/ytrain-2.csv').drop('Unnamed: 0',axis=1)\n",
        "x_test= pd.read_csv('/content/xtest-2.csv').drop('Unnamed: 0',axis=1)\n",
        "y_test= pd.read_csv('/content/ytest-2.csv').drop('Unnamed: 0',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "standing-going",
      "metadata": {
        "id": "standing-going"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ranking-louis",
      "metadata": {
        "id": "ranking-louis"
      },
      "outputs": [],
      "source": [
        "length=int(len(x_train)*0.1)\n",
        "x_valid=x_train.iloc[:10000,:]\n",
        "y_valid=y_train.iloc[:10000,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "precise-disabled",
      "metadata": {
        "id": "precise-disabled"
      },
      "outputs": [],
      "source": [
        "x_train=x_train.values\n",
        "y_train=y_train.values\n",
        "x_valid=x_valid.values\n",
        "y_valid=y_valid.values\n",
        "x_test=x_test.values\n",
        "y_test=y_test.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "serious-funds",
      "metadata": {
        "id": "serious-funds"
      },
      "outputs": [],
      "source": [
        "x_train = torch.tensor(x_train, dtype = float)\n",
        "y_train = torch.tensor(y_train, dtype = float)\n",
        "x_valid = torch.tensor(x_valid , dtype = torch.float)\n",
        "y_valid = torch.tensor(y_valid , dtype =torch.float)\n",
        "x_test = torch.tensor(x_test, dtype = torch.float)\n",
        "y_test = torch.tensor(y_test, dtype = torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mounted-alliance",
      "metadata": {
        "id": "mounted-alliance"
      },
      "outputs": [],
      "source": [
        "# not very clear about defining the model, not working here\n",
        "# ??\n",
        "input_size =x_train.shape[1]\n",
        "output_size = 5\n",
        "batch_size = 300\n",
        "class my_model(torch.nn.Module):\n",
        "    def __init__(self,input_size, output_size,l,w):\n",
        "\n",
        "       \n",
        "\n",
        "        self.linear_layers.append(torch.nn.ReLU())\n",
        "#??\n",
        "            \n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "       \n",
        "      \n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MSxN9dQ5VjH7"
      },
      "id": "MSxN9dQ5VjH7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dated-shopping",
      "metadata": {
        "id": "dated-shopping"
      },
      "outputs": [],
      "source": [
        "# should be same as before but with Adam optimizer, not sure\n",
        "def train(input_size, output_size,l,w,lr,epoch,batch_size,verbose=True):\n",
        "    my_nn=my_model(input_size, output_size,l,w)\n",
        "    cost = torch.nn.MSELoss(reduction='mean')\n",
        "    optimizer = torch.optim.Adam(my_nn.parameters(), lr = lr,eps=1e-3)\n",
        "\n",
        "    losses_valid = []\n",
        "    losses_test = []\n",
        "\n",
        "    for i in range(epoch):\n",
        "        batch_loss_valid = []\n",
        "        batch_loss_test = []\n",
        "\n",
        "        for start in range(0, len(x_train), batch_size):\n",
        "            end = start + batch_size if start + batch_size < len(x_train) else len(x_train)\n",
        "            xx = torch.tensor(x_train[start:end], dtype = torch.float, requires_grad = True)\n",
        "            yy = torch.tensor(y_train[start:end], dtype = torch.float, requires_grad = True)\n",
        "            prediction = my_nn(xx)\n",
        "            loss = cost(prediction, yy)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            optimizer.step()\n",
        "\n",
        "        prediction_valid = my_nn(x_valid)\n",
        "        l_valid=cost(prediction_valid, y_valid)\n",
        "\n",
        "\n",
        "        prediction_test = my_nn(x_test)\n",
        "        l_test=cost(prediction_test, y_test)\n",
        "\n",
        "       \n",
        "        losses_valid.append(l_valid.data.numpy())\n",
        "        losses_test.append(l_test.data.numpy())\n",
        "        if verbose==True:\n",
        "            print('valid loss for epoch '+str(i)+': '+str(l_valid.data.numpy()))\n",
        "            print('test loss for epoch '+str(i)+': '+str(l_test.data.numpy()))\n",
        "    return my_nn\n",
        "\n",
        "# following code for in different conditions should be similar, no result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "variable-robert",
      "metadata": {
        "id": "variable-robert"
      },
      "outputs": [],
      "source": [
        "# 2 5\n",
        "input_size =x_train.shape[1]\n",
        "output_size = 5\n",
        "batch_size = 300\n",
        "lr=1e-4\n",
        "l=2\n",
        "w=5\n",
        "epoch=5\n",
        "my_nn=train(input_size, output_size,l,w,lr,epoch,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "changing-excitement",
      "metadata": {
        "id": "changing-excitement"
      },
      "outputs": [],
      "source": [
        "# 2 10\n",
        "input_size =x_train.shape[1]\n",
        "output_size = 5\n",
        "batch_size = 300\n",
        "lr=1e-4\n",
        "l=2\n",
        "w=10\n",
        "epoch=5\n",
        "my_nn=train(input_size, output_size,l,w,lr,epoch,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "billion-scholarship",
      "metadata": {
        "id": "billion-scholarship"
      },
      "outputs": [],
      "source": [
        "# 2 100\n",
        "input_size =x_train.shape[1]\n",
        "output_size = 5\n",
        "batch_size = 300\n",
        "lr=1e-4\n",
        "l=2\n",
        "w=100\n",
        "epoch=5\n",
        "my_nn=train(input_size, output_size,l,w,lr,epoch,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "creative-butler",
      "metadata": {
        "id": "creative-butler"
      },
      "outputs": [],
      "source": [
        "# 2 200\n",
        "input_size =x_train.shape[1]\n",
        "output_size = 5\n",
        "batch_size = 300\n",
        "lr=1e-4\n",
        "l=2\n",
        "w=200\n",
        "epoch=5\n",
        "my_nn=train(input_size, output_size,l,w,lr,epoch,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coordinate-timothy",
      "metadata": {
        "id": "coordinate-timothy"
      },
      "outputs": [],
      "source": [
        "# 3 5\n",
        "input_size =x_train.shape[1]\n",
        "output_size = 5\n",
        "batch_size = 300\n",
        "lr=1e-4\n",
        "l=3\n",
        "w=5\n",
        "epoch=5\n",
        "my_nn=train(input_size, output_size,l,w,lr,epoch,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "divided-anthony",
      "metadata": {
        "id": "divided-anthony"
      },
      "outputs": [],
      "source": [
        "# 3 10\n",
        "input_size =x_train.shape[1]\n",
        "output_size = 5\n",
        "batch_size = 300\n",
        "lr=1e-4\n",
        "l=3\n",
        "w=10\n",
        "epoch=5\n",
        "my_nn=train(input_size, output_size,l,w,lr,epoch,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "protecting-audience",
      "metadata": {
        "id": "protecting-audience"
      },
      "outputs": [],
      "source": [
        "# 3 100\n",
        "input_size =x_train.shape[1]\n",
        "output_size = 5\n",
        "batch_size = 300\n",
        "lr=1e-4\n",
        "l=3\n",
        "w=100\n",
        "epoch=5\n",
        "my_nn=train(input_size, output_size,l,w,lr,epoch,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "double-system",
      "metadata": {
        "id": "double-system"
      },
      "outputs": [],
      "source": [
        "# 3 200\n",
        "input_size =x_train.shape[1]\n",
        "output_size = 5\n",
        "batch_size = 300\n",
        "lr=1e-4\n",
        "l=3\n",
        "w=200\n",
        "epoch=5\n",
        "my_nn=train(input_size, output_size,l,w,lr,epoch,batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dangerous-yield",
      "metadata": {
        "id": "dangerous-yield"
      },
      "outputs": [],
      "source": [
        "# Didn't get the result but the intuition may be: as l and w increase, the loss would decrease"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "colab": {
      "name": "assessment1_Tianhao.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}